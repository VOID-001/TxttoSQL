Project-folder/
│
├── main.py                      # Entry point for FastAPI
├── routes/                      # FastAPI routes
│   ├── csv_routes.py             # CSV data upload and storage
│   ├── nlp_sql_routes.py         # SQL generation from natural language questions
│   ├── display_routes.py         # Display stored vectors/data
│   └── clear_routes.py           # Clear all vector embeddings from PGVector
├── utils/                       # Utility functions
│   ├── pgvector_client.py        # PGVector connection and utility methods
│   ├── embedding.py              # Embedding utilities (if needed)
│   └── train_sql_model.py        # Fine-tuning T5 for SQL generation
└── requirements.txt              # Project dependencies

├── main.py
from fastapi import FastAPI
from routes.csv_routes import csv_router
from routes.nlp_sql_routes import nlp_sql_router  # Renamed from diagnosis_routes
from routes.display_routes import display_router
from routes.clear_routes import clear_router

app = FastAPI()

# Include routers
app.include_router(csv_router, prefix="/api")
app.include_router(nlp_sql_router, prefix="/api")  # SQL generation route
app.include_router(display_router, prefix="/api")
app.include_router(clear_router, prefix="/api")

# Root route
@app.get("/")
def read_root():
    return {"message": "FastAPI with SQL Generation is running!"}

├── routes/
│   ├── csv_routes.py

from fastapi import APIRouter, UploadFile, File, HTTPException
import pandas as pd
from utils.pgvector_client import PGVectorClient
from utils.embedding import embedding_function

csv_router = APIRouter()

# Initialize PGVector Client (replace the credentials with your own)
pgvector_client = PGVectorClient("postgresql://postgres:password@localhost:5432/vector")

@csv_router.post("/upload-csv/")
async def upload_csv(file: UploadFile = File(...)):
    if not file.filename.endswith(".csv"):
        raise HTTPException(status_code=400, detail="Only CSV files are allowed.")

    try:
        df = pd.read_csv(file.file)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Error reading CSV file: {str(e)}")

    try:
        for _, row in df.iterrows():
            row_data = row.to_dict()
            row_text = " ".join([f"{key}: {value}" for key, value in row_data.items()])

            # Generate embedding
            embedding = embedding_function([row_text])[0].tolist()

            # Store in PGVector
            pgvector_client.add_embedding(embedding, row_data)

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error storing data: {str(e)}")

    return {"detail": "CSV data uploaded and embedded successfully"}

├── routes/
│   ├── nlp_sql_routes.py

from fastapi import APIRouter, HTTPException
from transformers import T5ForConditionalGeneration, T5Tokenizer
import os

nlp_sql_router = APIRouter()

# Load the fine-tuned model and tokenizer (ensure the path is correct)
model_path = "./MySQL_t5_model"
if not os.path.exists(model_path):
    raise Exception(f"Model not found at {model_path}. Please run the training first.")

model = T5ForConditionalGeneration.from_pretrained(model_path)
tokenizer = T5Tokenizer.from_pretrained(model_path)

# Hardcoded schema to be used in the model
static_schema = "employees(employee_id, name, department, position, salary, bike_owned)"

# Route for generating SQL query based on the natural language question
@nlp_sql_router.post("/generate-sql/")
async def generate_sql_query(question: str):
    try:
        # Combine schema and question
        input_text = f"translate English to SQL based on the provided schema:\nSchema: {static_schema}\nQuestion: {question}"

        # Preprocess the input question
        inputs = tokenizer.encode(input_text, return_tensors="pt", max_length=512, truncation=True)

        # Generate the output (SQL query)
        outputs = model.generate(inputs, max_length=150, num_beams=5, early_stopping=True)

        # Decode the output and return the SQL query
        sql_query = tokenizer.decode(outputs[0], skip_special_tokens=True)
        return {"sql_query": sql_query}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error generating SQL query: {str(e)}")

├── routes/
│   ├── display_routes.py

from fastapi import APIRouter, HTTPException
from utils.pgvector_client import PGVectorClient

display_router = APIRouter()

# Initialize PGVector Client
pgvector_client = PGVectorClient("postgresql://postgres:password@localhost:5432/vector")

@display_router.get("/display/")
async def display():
    try:
        # Fetch all data from PGVector
        all_data = pgvector_client.get_all_embeddings()

        if not all_data:
            return {"detail": "No data in the collection"}

        return {"data": all_data}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error fetching data: {str(e)}")

├── routes/
│   └── clear_routes.py

from fastapi import APIRouter, HTTPException
from utils.pgvector_client import PGVectorClient

clear_router = APIRouter()

# Initialize PGVector Client
pgvector_client = PGVectorClient("postgresql://postgres:password@localhost:5432/vector")

@clear_router.delete("/clear-vectordb/")
async def clear_vectordb():
    try:
        # Clear all embeddings from PGVector
        pgvector_client.clear_embeddings()
        return {"detail": "All data cleared from the vector database"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error clearing data: {str(e)}")

├── utils/
│   ├── pgvector_client.py

import psycopg2
import uuid

class PGVectorClient:
    def __init__(self, database_url):
        # Initialize connection to PostgreSQL database with PGVector
        self.conn = psycopg2.connect(database_url)
        self.cursor = self.conn.cursor()

    def add_embedding(self, embedding, metadata):
        # Generate UUID for the document
        document_id = str(uuid.uuid4())

        # SQL query to insert into vectors table
        insert_query = """
        INSERT INTO vectors (document_id, embedding, metadata)
        VALUES (%s, %s, %s);
        """
        self.cursor.execute(insert_query, (document_id, embedding, metadata))
        self.conn.commit()
        return document_id

    def get_all_embeddings(self):
        # SQL query to get all embeddings
        query = "SELECT document_id, embedding, metadata FROM vectors"
        self.cursor.execute(query)
        return self.cursor.fetchall()

    def clear_embeddings(self):
        # SQL query to clear all data from the vectors table
        self.cursor.execute("DELETE FROM vectors;")
        self.conn.commit()

    def close(self):
        # Close the database connection
        self.conn.close()

├── utils/
│   └── train_sql_model.py

from datasets import load_dataset
from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments
import torch

# Hardcoded schema
static_schema = "employees(employee_id, name, department, position, salary, bike_owned)"

# Preprocessing function for the dataset
def preprocess_function(examples):
    questions = examples['question']
    sql_queries = examples['query']

    # Prepend schema to every question
    questions_with_schema = [f"Schema: {static_schema}. Question: {question}" for question in questions]

    return {
        'input_text': questions_with_schema,
        'target_text': sql_queries
    }

# Training function
def train_sql_model():
    # Load dataset
    dataset = load_dataset("xlangai/spider")

    # Apply preprocessing
    tokenized_dataset = dataset.map(preprocess_function, batched=True)

    # Load the tokenizer and model
    model = T5ForConditionalGeneration.from_pretrained("google/flan-t5-small")
    tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-small")

    # Tokenize the input and target text
    def tokenize_function(examples):
        inputs = tokenizer(examples['input_text'], max_length=512, truncation=True, padding="max_length")
        targets = tokenizer(examples['target_text'], max_length=512, truncation=True, padding="max_length")

        inputs['labels'] = targets['input_ids']
        return {
            'input_ids': torch.tensor(inputs['input_ids'], dtype=torch.long),
            'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),
            'labels': torch.tensor(inputs['labels'], dtype=torch.long)
        }

    tokenized_dataset = tokenized_dataset.map(tokenize_function, batched=True, remove_columns=dataset['train'].column_names)

    # Split into training and validation sets
    if 'validation' not in tokenized_dataset:
        tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1)

    # Define training arguments
    training_args = TrainingArguments(
        output_dir="./my_static_schema_t5_model",
        evaluation_strategy="epoch",
        learning_rate=2e-5,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        num_train_epochs=3,
        save_strategy="epoch",
        logging_dir='./logs',
        logging_steps=10,
    )

    # Create Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset['train'],
        eval_dataset=tokenized_dataset['validation'],
    )

    # Fine-tune the model
    trainer.train()

    # Save the fine-tuned model and tokenizer
    trainer.save_model("./MySQL_t5_model")
    tokenizer.save_pretrained("./MySQL_t5_model")
    print("Model fine-tuned and saved at './MySQL_t5_model'")

├── utils/
│   ├── embedding.py

from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')

def embedding_function(texts):
    return model.encode(texts)
