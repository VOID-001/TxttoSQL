from transformers import pipeline
from utils.pgvector_client import collection
from utils.embedding import embedding_function

# Use a Hugging Face model for SQL generation
model = pipeline('text2text-generation', model='mrm8488/t5-base-finetuned-wikiSQL')


def retrieve_relevant_schema(prompt_embedding):
    """Retrieve relevant schema information from ChromaDB using the prompt embedding."""
    results = collection.query(query_embeddings=[prompt_embedding], n_results=3, include=["documents"])

    # Combine the top schema documents into a single string for context
    schema_context = " ".join([doc for doc in results['documents']])
    return schema_context


def generate_sql_query(prompt: str):
    # Embed the user prompt to search the vector database
    prompt_embedding = embedding_function([prompt])[0].tolist()

    # Retrieve relevant schema information
    schema_context = retrieve_relevant_schema(prompt_embedding)

    # Combine the prompt and schema context for model input
    contextual_prompt = f"Given this schema: {schema_context}, write an SQL query for: {prompt}"

    # Generate SQL query from the natural language prompt and schema context
    response = model(contextual_prompt, max_length=512, truncation=True)  # Add truncation length here

    # Extract and return the generated SQL query
    sql_query = response[0]['generated_text']
    return sql_query
