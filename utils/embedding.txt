from transformers import T5Tokenizer, T5Model
import torch

# Initialize the same model for embeddings and generation
model_name = 't5-base'
t5_model = T5Model.from_pretrained(model_name)
t5_tokenizer = T5Tokenizer.from_pretrained(model_name)


def embedding_function(texts):
    try:
        # Tokenize the input texts
        inputs = t5_tokenizer(texts, return_tensors='pt', padding=True, truncation=True)

        # Pass the tokenized text through the model to get the encoder hidden states
        with torch.no_grad():
            outputs = t5_model.encoder(**inputs)

        # Get the embeddings by averaging the encoder's last hidden state
        embeddings = torch.mean(outputs.last_hidden_state, dim=1)
        return embeddings.cpu().numpy()

    except Exception as e:
        raise RuntimeError(f"Error generating embeddings: {str(e)}")
